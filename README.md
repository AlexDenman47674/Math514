# Math514
Evaluation of University Performance: REF, REA and TEF

The Research Assessment Exercise (RAE) was first undertaken in 1986, as a UK-wide peer review exercise run on behalf of the four UK higher education funding bodies (The University of Edinburgh, 2016). This exercise was put in place to evaluate the quality of research within these institutions, utilising a 1-5* rating scale to measure quality (Council, 2013) – this has since been replaced. 

In 2008 this initial exercise became jointly conducted by the Higher education Funding Council for England (HEFCE), the Scottish Funding Council (SFC), the Higher Education Funding Council for Wales (HEFCW), and the Department for Employment and Learning, Northern Ireland (DEL) (The University of Edinburgh, 2016). Another alteration was made in 2014, when the Research Excellence Framework (REF) was established. 

The REF became the new system for assessing quality of research in UK higher education institutions, put in place to overtake the RAE (The University of St Andrews, 2014). This framework is now conducted every 4 years as a process of expert review (REF, 2021b). Expert panellists conduct the peer review for a total of 34 subject-based units of assessments (UOAs), using three weighted elements to form an overall scoring profile of each submission (The University of Sheffield, 2022).  

The three elements of which aid the assessment include: the quality of outputs, impact of our research beyond academia, and the environment that supports research (The University of Sheffield, 2022). The weightings of each element are as follows: Outputs 60%, Impact 25%, Environment 15% (REF, 2021a). The change from RAE to REF also saw an alternation in the rating system; each submitted item is graded at one of 5 levels, starting at ‘unclassified’ and increasing to ‘4*’ (The University of Sheffield, 2022). 

RAE and REF have three main aims: to provide accountability for public investment in research and produce evidence of the benefits of this investment; to provide benchmarking information and establish reputational yardsticks for use within the HE sector and for public information; to inform the selective allocation of funding for research (REF, 2021a). These aims aid in providing quality profiles of research activity across the HE sector (The University of Edinburgh, 2016) and provide reasoning for the selective allocation funding (REF, 2021b) – a total of £2 billion in public finding is invested in research annually (REF, 2021a).  

The 2021 REF reveals that 157 universities were assessed that year, an increase from the 2014 REF in which 154 universities were involved (REF, 2021a). Additionally, a comparison can be made regarding the proportion of submissions allocated within each grading level from 2014 to 2021. In 2014, 30% of submissions received a 4*, whilst 46% received a 3* grade (REF, 2014). In 2021, submissions receiving a 4* rating increased to 41%, whilst the proportion of 3* grades presented as 43% (REF, 2021a) – thus suggesting that the quality of submission has increased. In 2021, fewer submissions were placed in lower-level categories. This improvement of results in 2021 are suggestive of the fact that REF aims are being achieved as a higher proportion of submissions are achieving higher scores. Utilising this rating systems also aids in providing accountability for public investment, as the scoring systems helps in informing funding allocation decisions.  

A significant change that has been seen since the initial RAE in 1986, and the introduction of REF in 2014, is the alterations regarding the weighting of each assessment element. In 2014, the ‘impact’ element, accounted for only 20% of the overall profile rating, however this has since increased to 25% (McKay, 2017), and was utilised in the 2021 REF.  

Additionally, a real time REF review pilot was conducted during the 2021 REF assessment, to test feasibility of evaluating perceptions and experiences of the REF in real-time, among researchers at all career stages, and across a wide range of disciplines and universities (Weinstein and Wilsdon, 2018). 

As of April 2018, the management of REF changed to Research England, of whom are part of UK Research and Innovation (UKRI) (Weinstein and Wilsdon, 2018). The same year, this management trialled a more formative approach to better understand the effect of the framework on institutions and individuals (Weinstein and Wilsdon, 2018). 

Issues with REF 

Output driven research skews data and statistical soundness  

Grove identifies that 84% of UK institutions ranked were in the leading two quality profiles, with an overall of 76% in the leading two profiles, increasing overall sector GPA by 0.27 since 2014. Grove theorises that the increase in GPA is driven by universities focusing on research outputs, but is this is optimum system to measure research quality? 

Graphical user interface, application, table, Excel

Description automatically generatedThelwall et al found that 5 of 34 units of assessment have a weak tendency for higher scoring institutions to lose from fractional counting. 

A screenshot of a computer

Description automatically generated 

(Grove, 2022 -  info, table and pie chart) (Thelwall et al, 2021). 

 

Lack of peer review  

Sayer has criticised the REF method for falling short of international peer reviewing standards (Sayer, 2014).  

REF suffers from “bias or subjectivity” due to the evaluation process being skewed toward “research-intensive” universities (Pinar and Horne, 2022). However, any ranking of experiences will be inherently subjective, can this contemporary data issue be made fairer? 

 

Other approaches to measuring university performance 

Shifting focus away from research 

Other factors should be analysed such as research, teaching ad technology transfer (Gibari et al, 2018) as this gives a focus to the quality of teaching rather than subsequent research. 

Xia, Wu an Feng’s PCA analysis. 

PCA is a multivariate analysis that converts multiple indicators into fewer comprehensive indicators. Firstly, the data is standardized, then a dependency matrix of indicators is set, then the eigenvalue and eigenvector of matrix r is found and the principal component expression is found, next the variance contribution rate is found to determine the number of principle components. ​(Xia, Wu, & Feng, 2019)​ 

 

References:

Council, F. (2013). Research Assessment Exercise 1996. [online] www.data.gov.uk. Available at: https://www.data.gov.uk/dataset/bca473d0-fb05-4e2b-9dfd-78e1ecafaaba/research-assessment-exercise-1996 [Accessed 10 Apr. 2023]. 

McKay, S. (2017). Moving impact from 20% to 25% in REF. [online] rstudio-pubs-static.s3.amazonaws.com. Available at: https://rstudio-pubs-static.s3.amazonaws.com/305617_e2b4d3a880a648e3a19c9f5fc0185dab.html [Accessed 12 Apr. 2023]. 

REF (2014). Home : REF 2014. [online] ref.ac.uk. Available at: https://ref.ac.uk/2014/. 

REF (2021a). REF 2021 Key Facts. Available at: https://www.ref.ac.uk/media/1848/ref2021_key_facts.pdf [Accessed 12 Apr. 2023]. 

‌REF (2021b). What is the REF? - REF 2021. [online] Higher Education Funding Council for England. Available at: https://www.ref.ac.uk/about-the-ref/what-is-the-ref/. [Accessed 12 Apr. 2023]. 

The University of Edinburgh. (2016). About the Research Assessment Exercise. [online] Available at: https://www.ed.ac.uk/research/assessment/rae/about [Accessed 10 Apr. 2023]. 

The University of Sheffield (2022). What is REF? [online] www.sheffield.ac.uk. Available at: https://www.sheffield.ac.uk/research/what-ref [Accessed 12 Apr. 2023]. 

The University of St Andrews (2014). Research quality | Current Staff | University of St Andrews. [online] www.st-andrews.ac.uk. Available at: https://www.st-andrews.ac.uk/staff/research/quality/ [Accessed 10 Apr. 2023]. 

Weinstein, N. and Wilsdon, J. (2018). Why we need a Real-Time REF Review to plan for 2027. [online] Wonkhe. Available at: https://wonkhe.com/blogs/why-we-need-a-real-time-ref-review-to-plan-for-2027/ [Accessed 12 Apr. 2023]. 
